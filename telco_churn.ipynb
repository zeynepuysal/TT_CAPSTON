{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcombined_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1465\u001b[0m, in \u001b[0;36mpandas._libs.parsers._maybe_upcast\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\multiarray.py:1148\u001b[0m, in \u001b[0;36mputmask\u001b[1;34m(a, mask, values)\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;124;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n\u001b[1;32m-> 1148\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mputmask)\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mputmask\u001b[39m(a, \u001b[38;5;241m/\u001b[39m, mask, values):\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;124;03m    putmask(a, mask, values)\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \n\u001b[0;32m   1190\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, mask, values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing  import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 170)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtype == \"object\"]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtype != \"object\"]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtype == \"object\"]\n",
    "    cat_cols = list(set(cat_cols + num_but_cat) - set(cat_but_car))\n",
    "    \n",
    "  \n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtype in [\"int64\", \"float64\"]]\n",
    "    \n",
    "    return cat_cols, num_cols, cat_but_car\n",
    "cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
    "\n",
    "print(\"Kategorik Değişkenler:\", cat_cols)\n",
    "print(\"Sayısal Değişkenler:\", num_cols)\n",
    "print(\"Kardinalitesi Yüksek Kategorik Değişkenler:\", cat_but_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_csv(\"combined_data.csv\")\n",
    "postpaid_missing_auto_payment = df.filter(\n",
    "    (pl.col(\"service_type\") == \"Postpaid\") & (pl.col(\"auto_payment\").is_null())\n",
    ")\n",
    "print(f\"Postpaid olup auto_payment eksik olan satır sayısı: {postpaid_missing_auto_payment.shape[0]}\")\n",
    "print(postpaid_missing_auto_payment.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"combined_data.csv\")\n",
    "df_copy = df.copy()\n",
    "df_copy.loc[(df_copy[\"service_type\"] == \"Postpaid\") & (df_copy[\"auto_payment\"].isnull()) & (df_copy[\"overdue_payments\"] == 0), \"auto_payment\"] = \"True\"\n",
    "df_copy.loc[(df_copy[\"service_type\"] == \"Postpaid\") & (df_copy[\"auto_payment\"].isnull()) & (df_copy[\"overdue_payments\"] > 0), \"auto_payment\"] = \"False\"i\n",
    "df_copy[\"tenure\"].fillna(df_copy[\"tenure\"].median(), inplace=True)\n",
    "df_copy[\"data_usage\"].fillna(0, inplace=True)\n",
    "df_copy[\"avg_call_duration\"].fillna(df_copy[\"avg_call_duration\"].median(), inplace=True)\n",
    "df_copy[\"roaming_usage\"].fillna(0, inplace=True)\n",
    "df_copy[\"call_drops\"].fillna(0, inplace=True)\n",
    "df_copy[\"monthly_charge\"] = df_copy.groupby([\"service_type\", \"auto_payment\"])[\"monthly_charge\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "df_copy.to_csv(\"combined_data_filled.csv\", index=False)\n",
    "\n",
    "\n",
    "print(df_copy.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.loc[(df_copy[\"service_type\"] == \"Postpaid\") & (df_copy[\"auto_payment\"].isnull()) & (df_copy[\"overdue_payments\"] == 0), \"auto_payment\"] = \"True\"\n",
    "df_copy.loc[(df_copy[\"service_type\"] == \"Postpaid\") & (df_copy[\"auto_payment\"].isnull()) & (df_copy[\"overdue_payments\"] > 0), \"auto_payment\"] = \"False\"\n",
    "df_copy[\"auto_payment\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "grouped_median = df_copy.groupby([\"service_type\", \"auto_payment\"])[\"monthly_charge\"].median().reset_index()\n",
    "grouped_median.rename(columns={\"monthly_charge\": \"median_monthly_charge\"}, inplace=True)\n",
    "\n",
    "\n",
    "df_copy = df_copy.merge(grouped_median, on=[\"service_type\", \"auto_payment\"], how=\"left\")\n",
    "\n",
    "\n",
    "df_copy[\"monthly_charge\"].fillna(df_copy[\"median_monthly_charge\"], inplace=True)\n",
    "\n",
    "df_copy.drop(columns=[\"median_monthly_charge\"], inplace=True)\n",
    "\n",
    "\n",
    "df_copy[\"monthly_charge\"].fillna(df_copy[\"monthly_charge\"].median(), inplace=True)\n",
    "\n",
    "\n",
    "print(\"Güncellenmiş veri setindeki null değerler:\")\n",
    "print(df_copy.isnull().sum())\n",
    "\n",
    "df_copy.to_csv(\"combined_data_filled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
    "\n",
    "\n",
    "def label_encoder(dataframe, binary_col):\n",
    "    labelencoder = LabelEncoder()\n",
    "    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n",
    "    return dataframe\n",
    "\n",
    "binary_cols = [col for col in df.columns if df[col].dtype in [\"int64\", \"object\"] and df[col].nunique() == 2]\n",
    "\n",
    "for col in binary_cols:\n",
    "    df = label_encoder(df, col)\n",
    "\n",
    "\n",
    "cat_cols = [col for col in cat_cols if col not in binary_cols and col not in [\"churn\", \"auto_payment\"]]\n",
    "\n",
    "# Sonucu Kontrol Et\n",
    "print(\"Binary Değişkenler:\", binary_cols)\n",
    "print(\"Güncellenmiş Kategorik Değişkenler:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unknown_analysis = df_copy[df_copy[\"auto_payment\"] == \"Unknown\"].groupby([\"service_type\", \"overdue_payments\"]).size().reset_index(name=\"count\")\n",
    "print(\"'Unknown' değerlerinin dağılımı:\")\n",
    "print(unknown_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport pandas as pd\n",
    "\n",
    "\n",
    "df_copy = pd.read_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "for service_type in df_copy[\"service_type\"].unique():\n",
    "    for overdue in df_copy[\"overdue_payments\"].unique():\n",
    "  \n",
    "        mask = (df_copy[\"service_type\"] == service_type) & (df_copy[\"overdue_payments\"] == overdue) & (df_copy[\"auto_payment\"] == \"Unknown\")\n",
    "        \n",
    "   \n",
    "        auto_payments_in_group = df_copy.loc[(df_copy[\"service_type\"] == service_type) & (df_copy[\"overdue_payments\"] == overdue), \"auto_payment\"]\n",
    "        if not auto_payments_in_group.empty:\n",
    "            mode_value = auto_payments_in_group.mode()\n",
    "            if not mode_value.empty:\n",
    "                mode_value = mode_value[0]\n",
    "                if mode_value == \"Unknown\":\n",
    "                    mode_value = False\n",
    "                print(f\"service_type: {service_type}, overdue: {overdue}, mode_value: {mode_value}\")\n",
    "                df_copy.loc[mask, \"auto_payment\"] = mode_value\n",
    "\n",
    "df_copy.to_csv(\"combined_data_filled.csv\", index=False)\n",
    "remaining_unknown = (df_copy[\"auto_payment\"] == \"Unknown\").sum()\n",
    "print(f\"auto_payment sütununda kalan 'Unknown' değer sayısı: {remaining_unknown}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
    "def label_encoder(dataframe, binary_col):\n",
    "    labelencoder = LabelEncoder()\n",
    "    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "binary_cols = [col for col in df.columns if df[col].dtype in [\"int64\", \"object\"] and df[col].nunique() == 2]\n",
    "\n",
    "\n",
    "for col in binary_cols:\n",
    "    df = label_encoder(df, col)\n",
    "cat_cols = [col for col in cat_cols if col not in binary_cols and col not in [\"churn\", \"auto_payment\"]]\n",
    "print(\"Binary Değişkenler:\", binary_cols)\n",
    "print(\"Güncellenmiş Kategorik Değişkenler:\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoderme\n",
    "cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
    "def label_encoder(dataframe, binary_col):\n",
    "    labelencoder = LabelEncoder()\n",
    "    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n",
    "    return dataframe\n",
    "\n",
    "binary_cols = [col for col in df.columns if df[col].dtype in [\"int64\", \"object\"] and df[col].nunique() == 2]\n",
    "\n",
    "\n",
    "for col in binary_cols:\n",
    "    df = label_encoder(df, col)\n",
    "\n",
    "\n",
    "cat_cols = [col for col in cat_cols if col not in binary_cols and col not in [\"churn\", \"auto_payment\"]]\n",
    "\n",
    "\n",
    "print(\"Binary Değişkenler:\", binary_cols)\n",
    "print(\"Güncellenmiş Kategorik Değişkenler:\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "df_copy = pl.read_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "\n",
    "def label_encoder(dataframe, binary_col):\n",
    "    labelencoder = LabelEncoder()\n",
    "    dataframe = dataframe.with_column(pl.Series(binary_col, labelencoder.fit_transform(dataframe[binary_col].to_pandas())))\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "binary_cols = [col for col in df_copy.columns if df_copy[col].dtype in [pl.Int64, pl.Utf8] and df_copy[col].unique().len() == 2]\n",
    "\n",
    "for col in binary_cols:\n",
    "    df_copy = label_encoder(df_copy, col)\n",
    "\n",
    "\n",
    "df_copy = df_copy.to_pandas()\n",
    "df_copy = pd.get_dummies(df_copy, columns=[\"service_type\", \"apps\"], drop_first=True)\n",
    "df_copy = pl.DataFrame(df_copy)\n",
    "df_copy.write_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "print(\"One-Hot Encoding işlemi tamamlandı ve veriler kalıcı hale getirildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df_copy = pl.read_csv(\"combined_data_filled.csv\")\n",
    "df_copy = df_copy.slice(1, df_copy.shape[0] - 1)\n",
    "df_copy.write_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "print(\"Gereksiz ilk satır silindi ve veriler kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import re\n",
    "\n",
    "df_copy = pl.read_csv(\"combined_data_filled.csv\")\n",
    "df_copy = df_copy.slice(1, df_copy.shape[0] - 1)\n",
    "def clean_column_names(column_names):\n",
    "    cleaned_names = []\n",
    "    for col in column_names:\n",
    "        cleaned_col = re.sub(r'[^A-Za-z0-9_]+', '_', col)\n",
    "        cleaned_names.append(cleaned_col)\n",
    "    return cleaned_names\n",
    "\n",
    "df_copy.columns = clean_column_names(df_copy.columns)\n",
    "y = df_copy[\"churn\"].to_pandas()\n",
    "X = df_copy.drop([\"churn\", \"id\"]).to_pandas()\n",
    "stratified_sample_size = sample_size // 2\n",
    "\n",
    "X_churn_1 = X[y == 1]\n",
    "y_churn_1 = y[y == 1]\n",
    "X_churn_0 = X[y == 0]\n",
    "y_churn_0 = y[y == 0]\n",
    "\n",
    "X_churn_1_sample, _, y_churn_1_sample, _ = train_test_split(X_churn_1, y_churn_1, train_size=stratified_sample_size, random_state=12345)\n",
    "X_churn_0_sample, _, y_churn_0_sample, _ = train_test_split(X_churn_0, y_churn_0, train_size=stratified_sample_size, random_state=12345)\n",
    "\n",
    "\n",
    "X_sample = pd.concat([X_churn_1_sample, X_churn_0_sample], axis=0)\n",
    "y_sample = pd.concat([y_churn_1_sample, y_churn_0_sample], axis=0)\n",
    "X_sample.columns = clean_column_names(X_sample.columns)\n",
    "\n",
    "print(f\"Seçilen örneklem boyutu: {X_sample.shape}\")\n",
    "print(f\"Churn sınıflarının dağılımı:\\n{y_sample.value_counts()}\")\n",
    "\n",
    "models = [\n",
    "    ('LR', LogisticRegression(random_state=12345)),\n",
    "    ('CART', DecisionTreeClassifier(random_state=12345)),\n",
    "    ('RF', RandomForestClassifier(random_state=12345)),\n",
    "    ('SVM', SVC(gamma='scale', random_state=12345)),\n",
    "    ('XGB', XGBClassifier(random_state=12345)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=12345))\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    try:\n",
    "        cv_results = cross_validate(\n",
    "            model, X_sample, y_sample, cv=10, \n",
    "            scoring=[\"accuracy\", \"f1\", \"roc_auc\", \"precision\", \"recall\"], \n",
    "            error_score='raise'\n",
    "        )\n",
    "        print(f\"######## {name} ######\")\n",
    "        print(f\"Accuracy: {round(cv_results['test_accuracy'].mean(), 4)}\")\n",
    "        print(f\"AUC: {round(cv_results['test_roc_auc'].mean(), 4)}\")\n",
    "        print(f\"Recall: {round(cv_results['test_recall'].mean(), 4)}\")\n",
    "        print(f\"F1: {round(cv_results['test_f1'].mean(), 4)}\")\n",
    "        print(f\"Precision: {round(cv_results['test_precision'].mean(), 4)}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name} modelinde hata oluştu: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n",
    "    quartile1 = dataframe[col_name].quantile(q1)\n",
    "    quartile3 = dataframe[col_name].quantile(q3)\n",
    "    interquantile_range = quartile3 - quartile1\n",
    "    up_limit = quartile3 + 1.5 * interquantile_range\n",
    "    low_limit = quartile1 - 1.5 * interquantile_range\n",
    "    return low_limit, up_limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outlier (dataframe, col_name): \n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, col_name) \n",
    "    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None): \n",
    "        return True \n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_thresholds(dataframe, variable, q1=0.05, q3=0.95):\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, variable, q1, q3)\n",
    "    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n",
    "    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    print(col, check_outlier(df, col))\n",
    "    if check_outlier(df, col):\n",
    "        replace_with_thresholds(df, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_copy = pl.read_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "\n",
    "df = df_copy.to_pandas()\n",
    "\n",
    "\n",
    "df['call_center_calls_per_month'] = df['customer_support_calls'] / (df['tenure'] + 1)\n",
    "\n",
    "\n",
    "df_copy = pl.DataFrame(df)\n",
    "\n",
    "\n",
    "df_copy.write_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "print(\"Yeni sütun oluşturuldu ve veriler kalıcı olarak kaydedildi.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "df_copy = pl.read_csv(\"combined_data_filled.csv\")\n",
    "df = df_copy.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df['normalized_satisfaction_score'] = scaler.fit_transform(df[['satisfaction_score']])\n",
    "\n",
    "\n",
    "df['normalized_support_calls'] = scaler.fit_transform(df[['customer_support_calls']])\n",
    "\n",
    "df['customer_support_effectiveness_score'] = df['normalized_satisfaction_score'] * df['normalized_support_calls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "apps_columns = [col for col in df.columns if col.startswith(\"apps_\")]\n",
    "df['total_apps_used'] = df[apps_columns].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['AgeGroup'] = pd.cut(df['age'], bins=[0, 25, 50, 75, 100], labels=['Genç', 'Orta Yaşlı', 'Yaşlı', 'Çok Yaşlı'])\n",
    "df['months_until_next_contract'] = 12 - (df['tenure'] % 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_copy = pl.DataFrame(df)\n",
    "df_copy.write_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "print(\"Tüm özellikler oluşturuldu ve veriler kalıcı olarak kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_copy = pl.read_csv(\"combined_data_filled.csv\")\n",
    "df = df_copy.to_pandas()\n",
    "label_encoder = LabelEncoder()\n",
    "df['AgeGroupEncoded'] = label_encoder.fit_transform(df['AgeGroup'])\n",
    "\n",
    "df_copy = pl.DataFrame(df)\n",
    "df_copy.write_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "print(\"Yaş grubu sütunu Label Encoding ile dönüştürüldü ve veriler kalıcı olarak kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_validate\n",
    "df_copy = pl.read_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "df = df_copy.to_pandas()\n",
    "y = df[\"churn\"]\n",
    "X = df.drop([\"churn\", \"id\"], axis=1)\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, train_size=5000, stratify=y, random_state=17)\n",
    "rf_model = RandomForestClassifier(random_state=17)\n",
    "rf_params = {\n",
    "    \"max_depth\": [5, 8, None],\n",
    "    \"max_features\": [3, 5, 7, \"sqrt\", \"log2\"],\n",
    "    \"min_samples_split\": [2, 5, 8],\n",
    "    \"n_estimators\": [100, 200, 500]\n",
    "}\n",
    "\n",
    "\n",
    "rf_best_grid = GridSearchCV(rf_model, rf_params, cv=5, n_jobs=-1, verbose=True).fit(X_sample, y_sample)\n",
    "print(\"En iyi parametreler:\", rf_best_grid.best_params_)\n",
    "print(\"En iyi skor:\", rf_best_grid.best_score_)\n",
    "\n",
    "rf_final = RandomForestClassifier(**rf_best_grid.best_params_, random_state=17).fit(X, y)\n",
    "\n",
    "\n",
    "cv_results = cross_validate(rf_final, X, y, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n",
    "print(\"Test Accuracy:\", cv_results['test_accuracy'].mean())\n",
    "print(\"Test F1 Score:\", cv_results['test_f1'].mean())\n",
    "print(\"Test ROC AUC Score:\", cv_results['test_roc_auc'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_validate\n",
    "\n",
    "\n",
    "df_copy = pl.read_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "r\n",
    "df = df_copy.to_pandas()\n",
    "\n",
    "y = df[\"churn\"]\n",
    "X = df.drop([\"churn\", \"id\"], axis=1)\n",
    "\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, train_size=5000, stratify=y, random_state=17)\n",
    "xgboost_model = XGBClassifier(random_state=17)\n",
    "xgboost_params = {\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001],\n",
    "    \"max_depth\": [5, 8, 12, 15, 20],\n",
    "    \"n_estimators\": [100, 500, 1000],\n",
    "    \"colsample_bytree\": [0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "xgboost_best_grid = GridSearchCV(xgboost_model, xgboost_params, cv=5, n_jobs=-1, verbose=True).fit(X_sample, y_sample)\n",
    "print(\"En iyi parametreler:\", xgboost_best_grid.best_params_)\n",
    "print(\"En iyi skor:\", xgboost_best_grid.best_score_)\n",
    "\n",
    "xgboost_final = XGBClassifier(**xgboost_best_grid.best_params_, random_state=17).fit(X_sample, y_sample)\n",
    "\n",
    "\n",
    "cv_results = cross_validate(xgboost_final, X_sample, y_sample, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n",
    "print(\"Test Accuracy:\", cv_results['test_accuracy'].mean())\n",
    "print(\"Test F1 Score:\", cv_results['test_f1'].mean())\n",
    "print(\"Test ROC AUC Score:\", cv_results['test_roc_auc'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_validate\n",
    "\n",
    "df_copy = pl.read_csv(\"combined_data_filled.csv\")\n",
    "\n",
    "df = df_copy.to_pandas()\n",
    "\n",
    "df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "\n",
    "y = df[\"churn\"]\n",
    "X = df.drop([\"churn\", \"id\"], axis=1)\n",
    "\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, train_size=1000, stratify=y, random_state=17)\n",
    "lgbm_model = LGBMClassifier(random_state=17)\n",
    "lgbm_params = {\n",
    "    \"learning_rate\": [0.01, 0.1, 0.001],\n",
    "    \"n_estimators\": [100, 300, 500, 1000],\n",
    "    \"colsample_bytree\": [0.5, 0.7, 1]\n",
    "}\n",
    "lgbm_best_grid = GridSearchCV(lgbm_model, lgbm_params, cv=5, n_jobs=-1, verbose=True).fit(X_sample, y_sample)\n",
    "print(\"En iyi parametreler:\", lgbm_best_grid.best_params_)\n",
    "print(\"En iyi skor:\", lgbm_best_grid.best_score_)\n",
    "\n",
    "lgbm_final = LGBMClassifier(**lgbm_best_grid.best_params_, random_state=17).fit(X_sample, y_sample)\n",
    "\n",
    "cv_results = cross_validate(lgbm_final, X_sample, y_sample, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n",
    "print(\"Test Accuracy:\", cv_results['test_accuracy'].mean())\n",
    "print(\"Test F1 Score:\", cv_results['test_f1'].mean())\n",
    "print(\"Test ROC AUC Score:\", cv_results['test_roc_auc'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
